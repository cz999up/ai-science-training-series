(groqflow) chengze@groq-r01-gn-01:~/groqflow/proof_points/natural_language_processing/bert$ python bert_tiny.py
tokenizer_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 346/346 [00:00<00:00, 1.53MB/s]
vocab.txt: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 3.74MB/s]
special_tokens_map.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:00<00:00, 1.56MB/s]
config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 760/760 [00:00<00:00, 6.40MB/s]
pytorch_model.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 17.6M/17.6M [00:00<00:00, 269MB/s]



Building "bert_tiny"
    ✓ Exporting PyTorch to ONNX
    ✓ Optimizing ONNX file
    ✓ Checking for Op support
    ✓ Converting to FP16
    ✓ Compiling model
    ✓ Assembling model

Woohoo! Saved to ~/.cache/groqflow/bert_tiny
Preprocessing data.
/home/chengze/.local/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for sst contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/sst
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
Downloading builder script: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 9.13k/9.13k [00:00<00:00, 62.8MB/s]
Downloading readme: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 6.68k/6.68k [00:00<00:00, 30.6MB/s]
Downloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 6.37M/6.37M [00:01<00:00, 5.62MB/s]
Downloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 790k/790k [00:00<00:00, 1.62MB/s]
Generating train split: 100%|████████████████████████████████████████████████████████████████████████████████████████| 8544/8544 [00:00<00:00, 11427.26 examples/s]
Generating validation split: 100%|████████████████████████████████████████████████████████████████████████████████████| 1101/1101 [00:00<00:00, 2051.12 examples/s]
Generating test split: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2210/2210 [00:00<00:00, 3904.41 examples/s]

Info: No inputs received for benchmark. Using the inputs provided during model compilation.
/home/chengze/groqflow/groqflow/groqmodel/execute.py:87: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  return tsp_runner(**example)
Running inference on GroqChip.
/home/chengze/groqflow/groqflow/groqmodel/execute.py:87: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  return tsp_runner(**example)
Running inference using PyTorch model (CPU).
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2210/2210 [00:04<00:00, 482.98it/s]
+--------+----------+-------------------------+----------------+----------------------+-------------+
| Source | Accuracy | end-to-end latency (ms) | end-to-end IPS | on-chip latency (ms) | on-chip IPS |
+--------+----------+-------------------------+----------------+----------------------+-------------+
|  cpu   |  77.47%  |           2.07          |     482.88     |          --          |      --     |
|  groq  |  77.47%  |           0.07          |    13832.75    |         0.03         |   32358.97  |
+--------+----------+-------------------------+----------------+----------------------+-------------+
