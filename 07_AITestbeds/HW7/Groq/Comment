# Maximun length was changed into 256 instead of 128 in the dummy section, shown below. But no significant affect on accuracy was found.
# dummy inputs to generate the groq model 
batch_size = 1
    max_seq_length = 128
    dummy_inputs = {
        "input_ids": torch.ones(batch_size, max_seq_length, dtype=torch.long),
        "attention_mask": torch.ones(batch_size, max_seq_length, dtype=torch.bool),
    }
  # dummy inputs to generate the groq model
    batch_size = 1
    max_seq_length = 256
    desired_seq_length = 128

    dummy_inputs = {
                "input_ids": torch.ones(batch_size, max_seq_length, dtype=torch.long),
                    "attention_mask": torch.ones(batch_size, max_seq_length, dtype=torch.bool),
                    }

    # generate groq model
    groq_model = groqit(pytorch_model, dummy_inputs, rebuild=rebuild_policy)

    # compute performance on CPU and GroqChip
    if should_execute:
        return compute_performance(
            groq_model,
            pytorch_model,
            dataset="sst",
            tokenizer=tokenizer,
            max_seq_length = min(max_seq_length, desired_seq_length),
            task="classification",
        )
